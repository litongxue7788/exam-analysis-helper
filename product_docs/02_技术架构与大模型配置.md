试卷分析助手 - 大模型配置与管理技术说明

本说明面向后续接手开发的同事，重点描述「大模型调用配置」和「管理员在界面配置秘钥」这一块的整体设计与代码位置，方便维护和扩展。

---

零、最高宪法与开发原则（大模型相关）

最高宪法：大模型能力必须服务于“学生知识增长 + 方法增长（学会学习）”的闭环，任何调用都要能落到“诊断 → 行动 → 复测验证 → 复盘沉淀”中的至少一步。

开发原则：

- 秘钥不出后端：前端不保存、不传递真实 API Key
- 默认安全：日志不输出秘钥；配置文件不应被提交到仓库
- 成本可控：对外使用必须可限流、可设超时、可降级
- 失败可解释：对用户返回可理解的错误信息，对管理员保留可追溯线索
- 可配置可回滚：配置变更可持久化、可恢复到上一次可用状态

---

一、整体目标与角色

- 管理目标
  - 所有大模型调用（结构化分析 + 图片分析）都由后端统一管理秘钥。
  - 管理者可以在前端界面上输入一次 API Key / 模型 ID 并保存到服务器。
  - 任课老师只负责上传试卷、生成报告，不需要也看不到任何秘钥。
- 角色
  - 管理员：配置大模型服务商（豆包 / 通义 / 智谱）的 Key 和模型 ID。
  - 老师：使用工具生成学生分析报告。

当前实现的是「单实例、本地部署」下的最简版本，为后续多学校、多 Key 管理预留了空间。

---

二、配置加载与优先级

1. 环境变量（.env）

- 文件：`backend/.env`
- 相关字段：
  - `DOUBAO_API_KEY` / `DOUBAO_BASE_URL` / `DOUBAO_MODEL_ID`
  - `ALIYUN_API_KEY` / `ALIYUN_BASE_URL` / `ALIYUN_MODEL_ID`
  - `ZHIPU_API_KEY` / `ZHIPU_BASE_URL` / `ZHIPU_MODEL_ID`
  - `DEFAULT_PROVIDER`：`doubao | aliyun | zhipu`
  - `ADMIN_PASSWORD`：管理员操作前端配置界面时需要输入的密码
- 作用：
  - 作为启动时的默认配置来源，尤其适用于首次部署或没有 `llm.json` 时。

2. 本地配置文件（llm.json）

- 文件路径：`config/llm.json`（运行时由后端创建 / 更新）
- 结构示例：

```json
{
  "doubao": {
    "apiKey": "your_api_key",
    "model": "your_model_id",
    "baseURL": "https://ark.cn-beijing.volces.com/api/v3"
  },
  "defaultProvider": "doubao"
}
```

- 加载逻辑：
  - 位置：`backend/server.ts` 中 `loadLlmConfigFromFile` 函数。
  - 服务启动时：
    - 如果存在 `llm.json`：
      - 读取文件并解析 JSON。
      - 对每个 provider（doubao/aliyun/zhipu）调用 `llmService.setProviderConfig` 覆盖内存中的配置。
      - 如果存在 `defaultProvider`，写入 `process.env.DEFAULT_PROVIDER`。
    - 如果不存在 `llm.json`：
      - 保持使用 `.env` 中的配置。

3. 生效优先级

1）服务启动时：
- 若有 `config/llm.json` → 优先用此文件内容覆盖运行时配置。
- 若无 `llm.json` → 使用 `.env` 中的配置。

2）运行过程中：
- 管理员通过前端配置界面保存后：
  - 内存中的配置立即更新（`llmService.setProviderConfig`）。
  - 同时写回 `config/llm.json`，作为后续重启时的持久化来源。

---

三、后端大模型服务层设计

1. 类型与接口定义

- 文件：`backend/llm/interface.ts`
- 核心类型：
  - `LLMProvider = 'doubao' | 'aliyun' | 'zhipu'`
- 核心接口：

```ts
export interface LLMService {
  generateAnalysis(prompt: string, modelProvider: LLMProvider): Promise<string>;
  generateImageAnalysis(images: string[], prompt: string, provider: LLMProvider): Promise<string>;
}
```

2. 实现类与配置结构

- 文件：`backend/llm/service.ts`

```ts
interface ProviderConfig {
  apiKey: string;
  baseURL: string;
  model: string;
}

export class LLMServiceImpl implements LLMService {
  private configs: Record<LLMProvider, ProviderConfig>;

  constructor() {
    this.configs = {
      doubao: {
        apiKey: process.env.DOUBAO_API_KEY || '',
        baseURL: process.env.DOUBAO_BASE_URL || 'https://ark.cn-beijing.volces.com/api/v3',
        model: process.env.DOUBAO_MODEL_ID || '',
      },
      aliyun: {
        apiKey: process.env.ALIYUN_API_KEY || '',
        baseURL: process.env.ALIYUN_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1',
        model: process.env.ALIYUN_MODEL_ID || 'qwen-plus',
      },
      zhipu: {
        apiKey: process.env.ZHIPU_API_KEY || '',
        baseURL: process.env.ZHIPU_BASE_URL || 'https://open.bigmodel.cn/api/paas/v4',
        model: process.env.ZHIPU_MODEL_ID || 'glm-4',
      }
    };
  }
```

- 动态配置接口：

```ts
getProviderConfig(provider: LLMProvider): ProviderConfig {
  return this.configs[provider];
}

setProviderConfig(provider: LLMProvider, config: Partial<ProviderConfig>): void {
  this.configs[provider] = {
    ...this.configs[provider],
    ...config,
  };
}
```

- 调用方法：
  - 文本分析：`generateAnalysis(prompt, provider)`
  - 图片分析：`generateImageAnalysis(images, prompt, provider)`
  - 两者都会检查对应 provider 的 `apiKey` 是否存在，否则抛出错误要求检查配置。

---

四、后端管理接口设计

1. 管理接口路径与行为

- 文件：`backend/server.ts`
- 路径：`POST /api/admin/llm-config`
- 请求体示例：

```json
{
  "adminPassword": "xxxx",
  "provider": "doubao",
  "apiKey": "sk-xxx",
  "modelId": "ep-xxx",
  "baseURL": "https://ark.cn-beijing.volces.com/api/v3"
}
```

- 处理逻辑：
  1. 读取 `process.env.ADMIN_PASSWORD`，若未配置 → 返回 500。
  2. 校验 `adminPassword` 是否匹配：
     - 不匹配 → 返回 401 + 错误信息“管理员密码错误”。
  3. 确定目标 provider：
     - 若请求体有 `provider` → 使用之；
     - 否则使用 `DEFAULT_PROVIDER`；
     - 若都没有 → 默认 `doubao`。
  4. 调用 `llmService.getProviderConfig(p)` 获取当前配置。
  5. 调用 `llmService.setProviderConfig(p, {...})` 更新内存中的配置。
  6. 读取现有 `config/llm.json`（若存在），更新对应 provider 的字段，并写回文件。
  7. 返回 `{ success: true }`。

2. 配置载入函数

- 函数：`loadLlmConfigFromFile`
- 作用：
  - 服务启动时检查并加载 `config/llm.json`，覆盖内存配置；
  - 设置 `process.env.DEFAULT_PROVIDER`（若有）。

---

五、前端管理界面设计

1. 入口位置

- 文件：`frontend/web/src/components/SettingsModal.tsx`
- 状态字段：
  - `isAdminMode`：是否展开管理员配置区。
  - `adminPassword`：管理员密码输入。
  - `adminMessage`：操作反馈信息（成功 / 失败）。
  - `adminSaving`：保存中状态。

2. 老师视角

- 设置弹窗中有一行说明：
  - 文本：`大模型服务由管理员统一配置，老师无需设置。`
  - 同时提供一个“小按钮”：`管理员配置`
  - 普通老师不需要点击即可正常使用分析功能。

3. 管理员视角（展开后）

- 展开管理员区域后，显示以下字段：
  1. 模型服务商选择（`ModelSelector`，与业务分析页共享）
  2. 管理员密码输入框（用于校验 `ADMIN_PASSWORD`）
  3. 模型 ID / 接入点 ID 输入框
  4. Base URL 输入框（可选，默认官方地址）
  5. API Key 输入框（以密码形式显示）
  6. 保存按钮：
     - 文案：`保存大模型配置到服务器`
     - 点击后请求 `POST /api/admin/llm-config`
     - 根据响应在下方展示成功或错误提示文本。

4. 与老师使用的关系

- 老师侧调用分析接口时，不需要也不传递任何 API Key：
  - 文字分析接口使用当前 `DEFAULT_PROVIDER` 与后端配置。
  - 图片分析接口仅传 `provider`（服务商标识），不传 Key：

```ts
const payload = {
  images: base64Images,
  provider: llmConfig.provider,
};
```

- 后端基于当前 provider 的配置（内存 + llm.json /.env）进行真实调用。

---

五点五、私人助理对话能力（规划，与产品方案同步）

定位：私人助理不是闲聊机器人，而是“学习教练 + 沟通翻译器”。其大模型调用同样遵循本项目最高宪法：服务于“知识增长 + 方法增长”的学习闭环。

1. 接口形态（建议）

- 路径：`POST /api/assistant/chat`
- 入参建议：
  - `mode`：`student | parent | teacher`（同一事实多视角表达）
  - `message`：用户输入
  - `context`：可选，当前报告摘要/学科年级/本周目标等（不包含敏感信息）
  - `provider`：可选，未传则使用 `DEFAULT_PROVIDER`
- 出参建议（结构化 JSON，便于前端渲染卡片而不只是文本）：
  - `reply`：可直接展示的回复
  - `actions`：下一步动作列表（任务卡/练习/复测）
  - `followUps`：引导用户继续闭环的追问列表

2. 隐私与记忆策略（阶段 1 默认）

- 默认不在服务器持久化保存对话；对话历史仅保存在浏览器本地
- 后端日志不记录用户原始对话全文；严禁输出或记录任何秘钥/管理员密码
- 若需要“短期记忆”，优先使用前端携带有限窗口上下文（例如最近 3–5 轮对话摘要）

3. 成本与稳定性策略

- 复用现有：限流、口令校验、超时、降级与解析鲁棒性能力
- 控制输入长度与上下文窗口，避免长对话导致成本失控
- 要求模型严格输出 JSON，解析失败时使用“修复提示词”进行一次修复重试

---

六、未来扩展建议

1. 多环境支持

- 目前设计适合单机部署（本地电脑 / 单台服务器）。
- 若未来采用多环境（dev / test / prod）：
  - 建议使用不同的 `.env` 与 `llm.json`。
  - 可在配置中增加 `env` 字段或根据 NODE_ENV 区分文件。

2. 授权码 / 租户机制

- 若将来要为不同学校 / 班级分配不同 Key 或额度：
  - 可在后端引入“租户 / 授权码”字段；
  - 老师首次登录输入授权码 → 服务端关联到特定 provider 配置；
  - 同样避免在前端暴露真实 Key。

3. 安全与审计

- 建议在生产环境中：
  - 使用 `.gitignore` 忽略 `config/llm.json`，避免误提交到仓库；
  - 控制能访问 `/api/admin/llm-config` 的网络范围（如仅内网、仅特定 IP）；
  - 后续可以增加操作日志记录（谁在什么时候更新了配置）。

---

七、快速排错指南

1. 报错：“未配置 xxx 的 API Key，请检查 .env 文件”

- 意味着对应 provider 的 `apiKey` 为空。
- 检查项：
  - `.env` 中是否配置了对应字段；
  - 若使用了管理界面更新，确认 `config/llm.json` 中对应字段是否存在且不为空；
  - 若刚修改 `.env`，是否已重启后端服务。

2. 报错：“管理员密码错误”

- 检查 `.env` 中 `ADMIN_PASSWORD` 是否与前端输入一致。
- 修改 `ADMIN_PASSWORD` 后需要重启后端服务。

3. 报错：“保存失败，请检查密码和配置”

- 可能原因：
  - 管理接口返回非 200 状态；
  - 响应中 `success` 为 false；
  - 或写入 `llm.json` 失败（如没有写权限）。
- 建议：
  - 查看后端控制台日志，查找 `❌ 管理接口处理失败` 相关输出；
  - 检查 `config` 目录是否存在，当前用户是否有写入权限。

---

八、技术迭代路径（大模型配置与稳定性）

1. 阶段 1：稳定可控（当前）

- 单实例、本地/单台服务器部署
- 管理员配置 Key 与模型，老师侧无感使用
- 强化超时、降级、限流与解析鲁棒性，保证试点稳定

2. 阶段 2：组织化试点（校内可复制）

- 引入“试点授权码/口令”与额度统计，便于成本控制与分角色使用
- 增加基础审计记录（不记录秘钥，只记录变更时间、操作者、provider、模型）
- 配置回滚：保留上一份可用配置，出现故障可快速恢复

3. 阶段 3：多租户与规模化

- 不同学校/年级/班可绑定不同 provider 配置与额度策略
- 引入任务队列/异步处理（尤其是图片分析），缓解高并发与超时
- 更完善的告警与看板（成功率、超时率、单次成本、日额度消耗）

---

九、产品试用场景与配置建议（与试点 SOP 对齐）

- 场景：考试/作业后 24 小时内生成报告与任务卡，讲评后 3–7 天做复测验证
- 配置建议：
  - 为试点先选定 1 个默认 provider 与模型，减少变量
  - 设置合理超时与限流（避免手机端连续上传导致拥塞）
  - 在对外试用前，先用管理员账号完成一次配置保存，并重启后端验证生效

本说明随代码一起维护，若后续对大模型服务商适配、配置方式或管理界面有调整，建议同步更新此文档以便交接。

---

十、视觉判题闭环（V0.1 方案口径）

> 本节用于把“视觉大模型判题 + 生成训练 + 验收”收敛为可实现、可维护、可控成本的技术口径。

### 1. 输入形态（V0.1 收敛）

- 默认以“单题/单页/作文片段/阅读题答案片段”作为图片输入，避免 V0.1 就承诺整卷切题与题号定位。
- 前端应在请求中携带最少但关键的元数据：
  - `subject`（数学/语文/英语）
  - `grade`（初三/高三等，用于语气与难度）
  - `provider`（模型服务商标识，不含 Key）
- 可选增强（不改变主链路）：前端或本地工具如已拿到 OCR 文本，可额外传 `ocrTexts`（string[]，按页/按块），后端在 Extract 阶段优先走“文本提取”，失败或低置信度再回退到图片提取。

### 2. 输出形态（必须结构化）

- 视觉判题输出必须是结构化 JSON，至少包含：
  - `confidence`（高/中/低）
  - `evidence`（支持结论的文本证据或卷面依据，不允许无证据下结论）
  - `errorTags`（错因标签，面向训练闭环而非泛泛知识点）
  - `nextActions`（最短改法、训练任务、验收规则）
- 当 `confidence` 为低时，必须返回“补拍/老师确认”的引导路径，不强行给出确定性结论。

### 3. 合规与反抄答案口径

- 默认不直接输出完整解题步骤或作文终稿。
- 若提供提示，必须分层解锁，并在使用高层提示后强制进入验收，避免形成抄答案路径。

### 4. 当前实现的最小对齐点

- 当前后端已提供图片分析入口：
  - `POST /api/analyze-images`
  - 入参：`images`（Data URI 数组）、`provider`、`subject`、`grade`
  - 出参：结构化报告与练习卷字段（`practicePaper` / `practiceQuestions`）
- V0.1 的视觉闭环可在该接口基础上迭代：优先强化输出的“证据 + 置信度 + 可执行下一步”，并确保生成的练习卷题目“同型 + 变式 + 迁移”结构稳定。

### 5. V0.2 以后建议的接口拆分（规划）

- 目的：把“整卷分析（报告）”与“逐题判题（闭环）”分离，降低复杂度并提升可控性。
- 建议新增（规划）：
  - `POST /api/grade-submission`：输入单题图片 +（可选）答案/评分点，输出判题结果（含证据/置信度/错因/提示链/验收规则）。
  - `POST /api/generate-practice-set`：输入错因与目标，输出题组（同型/变式/迁移）与验收小测。
- 对主观题（作文/阅读/英语写作）建议支持可选的 `rubric`（评分点/维度），用于提升一致性并允许教师改判后沉淀口径。

---

十一、分析时长优化（终极目标：快且不减产出）

> 结论：分析时长的决定因素不是“提示词更长更细”，而是把“信息获取（视觉/OCR）”与“推理/生成”解耦，并用多模型分工、并行、缓存与分段交付来把体验做到极致。

### 1. 用户体验 SLA（以用户视角验收）

- **0–3s**：前端必须有明确反馈（排队/进度/预计耗时），不可“无响应”
- **≤20s（P50）/ ≤45s（P95）**：先交付“可用核心结论”（摘要 + Top 错因 + 最短改法 + 证据/置信度）
- **≤90s（P95）**：交付“完整报告”（含 7 天计划、练习卷、验收小测、家长沟通话术等）
- **不压缩最终输出**：最终报告的丰富度不下降；改变的是“生成顺序”和“分工方式”

### 2. 最优架构：多模型分工协作 + 分段交付 + 统一整合

将当前“一次性 Vision 输出大 JSON”的链路拆为 4 段，并允许并行：

1）**Extract（事实底座，优先快）**

- 输入：图片（或 OCR 文本）
- 输出：小而严格的 JSON（题型/题号/得分点/错因候选/证据引用/置信度）
- 目标：尽快产出“事实”，供后续所有生成模块复用
- 门控原则：OCR 只用于“印刷体/明确字段”的提取；手写体默认不强求，低置信度直接提示补拍/老师确认，避免误判扩散到 Diagnose/Practice

2）**Diagnose（核心报告，优先准）**

- 输入：Extract 结构化事实 + 学科/年级
- 输出：核心诊断（forStudent/forParent 的关键段落 + 最短改法 + 证据/置信度）

3）**Practice（训练与验收，优先可执行）**

- 输入：弱点列表（来自 Extract/Diagnose）
- 输出：practicePaper + acceptanceQuiz
- 策略：允许后台补齐，不阻塞核心报告可读

4）**Merge（整合器，优先稳）**

- 输入：Extract + Diagnose + Practice
- 输出：与现有接口一致的最终数据结构（保证前端兼容与体验不缩水）
- 规则：整合器只做拼装与校验，不再长篇“重生成”

### 3. 分流策略（充分发挥大模型能力，同时缩短总时长）

- **任务路由（按任务选模型）**
  - Vision/Extract：用更快更稳的视觉模型做“抽取”
  - Diagnose：用推理更强的文本模型做“解释与建议”
  - Practice：用生成效率高/成本更优的模型做“出题与提示链”
- **选择性高算力（不确定处才用强模型）**
  - 只对低置信度或 Top-K 关键弱点进行深推理与题目生成
  - 其余弱点先给“最短改法”，题组后台补齐
- **并行与分块（Map/Reduce 思路）**
  - 多页图片按页/按 1–2 页切分并行 Extract，最后 Reduce 合并去重
- **强约束结构化输出**
  - 每段输出更小、schema 更明确，降低“修复 JSON 二次调用”的概率
- **缓存与去重**
  - 对图片内容做 hash：重复分析/重试/切换模型时复用 Extract 结果
  - 对“弱点+年级学科”的练习生成做短期缓存（例如 24h）

### 4. 与当前代码的衔接点（便于落地）

- 已有图片分析作业接口与 SSE 事件通道：
  - `POST /api/analyze-images/jobs` 创建作业
  - `GET /api/analyze-images/jobs/:jobId/events` 监听事件
- 当前作业 stage 已落地为：
  - `queued` → `extracting` → `diagnosing` → `practicing` → `merging` → `completed`
- 分段交付已落地：
  - `diagnosing` 完成后推送 `partial_result`（摘要 + Top 错因 + 最短改法 + 证据/置信度）
  - `practicing/merging` 完成后推送最终 `result`
- 缓存已落地：
  - 后端按图片内容 hash +（可选）OCR 文本 hash + 学科/年级 + 分段 provider 组合生成 cacheKey，复用完整结果
  - 可通过环境变量 `IMAGE_ANALYZE_CACHE_TTL_MS` 控制缓存 TTL
- 性能与稳定性相关环境变量（建议写入部署环境）：
  - `MAX_CONCURRENT_IMAGE_JOBS`：同一进程并发作业数上限
  - `JOB_EVENT_BUFFER_SIZE`：SSE 事件缓冲条数
  - `JOB_TTL_MS`：作业对象在内存中的保留时间
  - `LLM_TIMEOUT_MS` / `LLM_RETRY_COUNT` / `LLM_RETRY_BASE_DELAY_MS`：调用超时与重试策略
